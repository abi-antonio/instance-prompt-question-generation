# -*- coding: utf-8 -*-
"""UnifiedQA  Model Inference

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1akTDF2Q5GzTWI9kTqGBIjfYNjiFQcM45?usp=sharing

This notebook uses a QA model trained on single hop questions to answer multi-hop questions using decomposed single hop questions
"""

import os
import re
import sys
import json
import shutil
import logging
import numpy as np
from tqdm import tqdm
from random import shuffle, sample
from datetime import datetime, timezone
from argparse import ArgumentParser

import torch
from datasets import load_dataset, Dataset
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoConfig
from peft import PeftConfig, PeftModel
from sentence_transformers import SentenceTransformer, CrossEncoder
from sklearn.metrics.pairwise import cosine_similarity
from evaluate import load as eval_load
from InstructorEmbedding import INSTRUCTOR

from src.modeling_idpg import IDPGForMHQA
from utils import musique_eval_script, init_logger, compute_retrieval_metric
from musique.metrics.answer import AnswerMetric

logger = logging.getLogger(__name__)

def get_details(input, step_idx):
    json_obj = json.loads(input)

    if step_idx < len(json_obj['question_decomposition_preds']):
        id = json_obj['id']
        question = json_obj['question_decomposition_preds'][step_idx]['question']
        gold_answer = json_obj['question_decomposition'][step_idx]['answer'] if step_idx < len(json_obj['question_decomposition']) else ''

        # update references in question with predicted answer of previous question
        if step_idx != 0:
            references = re.findall("(#\d+)", question)
            # logger.info(f"{references=}")
            if len(references) > 0:
                for idx in references:
                    ref_step = int(idx[1:]) - 1
                    if ref_step >= len(json_obj['question_decomposition_preds']): # wrong reference or reference does not exist
                        continue

                    step_obj = json_obj['question_decomposition_preds'][int(idx[1:]) - 1]
                    if "predicted_answer" in step_obj.keys():
                        prev_ans = step_obj["predicted_answer"]
                        question = re.sub(f"{idx}", prev_ans, question)

        if use_gold_context:
            all_c_ids = list(range(len(json_obj['paragraphs'])))
            context_ids = []
            if step_idx < len(json_obj['question_decomposition']):
                gold_context_id = json_obj['question_decomposition'][step_idx]['paragraph_support_idx']
                context_ids = [gold_context_id]
                all_c_ids.remove(gold_context_id)
                
            if top_k_context >= len(json_obj['paragraphs']):
                add_p_ids = all_c_ids
                context_ids = context_ids + add_p_ids
            else:
                num_p = top_k_context - len(context_ids)
                if num_p > 0:
                    add_p_ids = sample(all_c_ids,num_p)
                    context_ids = context_ids + add_p_ids
            
            shuffle(context_ids)
        else:
            # get supporting paragraphs
            if top_k_context >= len(json_obj['paragraphs']):
                context_ids = [p["idx"] for p in json_obj['paragraphs']]
            else:
                if isinstance(sentence_model, CrossEncoder):
                    similarities = sentence_model.predict([(question, f"{p['title']}\n{p['paragraph_text']}") for p in json_obj['paragraphs']],
                                                        show_progress_bar=False)
                    if 'nboost' in ir_model_checkpoint:
                        similarities = similarities[:,1]
                        context_ids = np.argpartition(similarities, -top_k_context)[-top_k_context:].tolist()
                    else:
                        context_ids = np.argpartition(similarities, -top_k_context)[-top_k_context:].tolist()
                else:
                    if isinstance(sentence_model, INSTRUCTOR):
                        q_input = [['Represent the Wikipedia question for retrieving supporting documents: ', question]]
                        c_input = [['Represent the Wikipedia document for retrieval: ', f"{p['title']}\n{p['paragraph_text']}"] 
                                for p in json_obj['paragraphs']]
                    else:
                        q_input = [question]
                        c_input = [f"{p['title']}\n{p['paragraph_text']}" for p in json_obj['paragraphs']]
                    
                    q_embed = sentence_model.encode(q_input, show_progress_bar=False)
                    c_embeds = sentence_model.encode(c_input, show_progress_bar=False)

                    similarities = cosine_similarity(q_embed,c_embeds)
                    context_ids = np.argpartition(similarities, -top_k_context)[0,-top_k_context:].tolist()
                
        context_paragraphs = [f"{json_obj['paragraphs'][id]['title']}. {json_obj['paragraphs'][id]['paragraph_text']}" for id in context_ids]
        shuffle(context_paragraphs) # not shuffling is good
        context = "".join(context_paragraphs)
        
        return{'id':id,
            'question':question,
            'context':context,
            'context_ids': context_ids,
            'gold_answer': gold_answer,
            }

    return None

def build_hf_dataset(file_path: str, step_idx: int):
    logger.info(f"building dataset")
    with open(file_path, "r") as f:
        data=list()
        for line in tqdm(f.readlines()):
            data.append(get_details(line, step_idx))
        # data = [get_details(line, step_idx) for line in f]
        
        data = [d for d in data if d is not None]
        dataset = Dataset.from_list(data)

    return dataset

def qa_preprocess_function(examples):
    # QA inputs
    inputs = [f"{question}\n{context}"
                for context, question in list(zip(examples['context'], examples['question']))]

    model_inputs = tokenizer(inputs, max_length=8192, truncation=True)

    # Labels
    if 'answer' in examples.keys():
        model_inputs['labels'] = tokenizer(examples['answer'], max_length=100, truncation=True)['input_ids']

    return model_inputs

def qg_preprocess_function(examples):
    model_inputs = tokenizer(examples['question'], max_length=1024, truncation=True)

    # Prompter Inputs
    if 'idp' in qg_model_checkpoint:
        prompt_inputs = tokenizer(examples['question'], max_length=512, truncation=True)
        model_inputs["prompt_input_ids"] = prompt_inputs["input_ids"]
        model_inputs["prompt_attention_mask"] = prompt_inputs["attention_mask"]

    # Labels
    if 'question_decomposition' in examples.keys():
        label_inputs = [f"{tokenizer.sep_token}".join([hop["question"] for hop in sample]) for sample in examples['question_decomposition']]
        model_inputs['labels'] = tokenizer(label_inputs, max_length=100, truncation=True)['input_ids']

    return model_inputs

def generate_predictions(tokenized_val, model, skip_special_tokens=True, num_beams=1, mode='qa'):
    outputs = dict()
    for sample in tqdm(tokenized_val):

        if mode=='qg' and 'idp' in qg_model_checkpoint:
            model_output = model.generate(input_ids=torch.LongTensor([sample['input_ids']]).to(device),
                                                attention_mask=torch.LongTensor([sample['attention_mask']]).to(device),
                                                prompt_input_ids=torch.LongTensor([sample['prompt_input_ids']]).to(device),
                                                prompt_attention_mask=torch.LongTensor([sample['prompt_attention_mask']]).to(device),
                                                max_new_tokens=100,
                                                num_beams=num_beams,
                                                )
        else:
            model_output = model.generate(input_ids=torch.LongTensor([sample['input_ids']]).to(device),
                                                attention_mask=torch.LongTensor([sample['attention_mask']]).to(device),
                                                max_new_tokens=100,
                                                num_beams=num_beams,
                                                )
        pred_answer = tokenizer.decode(model_output[0], skip_special_tokens=skip_special_tokens)

        outputs[sample['id']] = {"pred_text": pred_answer}

    return outputs

def generate_qg_decompositions(val_dataset: Dataset, pred_out_path: str):
    global tokenizer

    SEP_TOKEN, EOS_TOKEN, PAD_TOKEN = '[SEP]', '</s>', ''
    if qg_model_checkpoint is not None:
        logger.info(f"loading model from {qg_model_checkpoint}")
        if 'idp' in qg_model_checkpoint: # and 'run' in qg_model_checkpoint:
            model = IDPGForMHQA.from_pretrained(qg_model_checkpoint)
            model.to(device)
            tokenizer = AutoTokenizer.from_pretrained(model.config.model_checkpoint)
            tokenizer.add_special_tokens({'sep_token':'[SEP]'})
        elif 'pt' in qg_model_checkpoint:
            config = PeftConfig.from_pretrained(qg_model_checkpoint)
            model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path)
            model = PeftModel.from_pretrained(model, qg_model_checkpoint)
            model = model.to(device)
            tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-large")
            tokenizer.add_special_tokens({'sep_token':'[SEP]'})
        else:
            model = AutoModelForSeq2SeqLM.from_pretrained(qg_model_checkpoint)
            model = model.to(device)
            tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-large")
            tokenizer.add_special_tokens({'sep_token':'[SEP]'})

    # use gold predictions
    if use_gold_shq:
        predictions = {id: {'pred_text': f"{SEP_TOKEN}".join([hop['question_clean'] for hop in sample])} 
                    for id, sample in list(zip(val_dataset['id'],val_dataset['question_decomposition']))}
    # get predictions from file
    elif args.qg_pred_path is not None:
        logger.info(f"loading decomposed questions from {args.qg_pred_path}")
        predictions = dict()
        with open(args.qg_pred_path, "r") as f:
            for line in tqdm(f.readlines()):
                json_obj = json.loads(line)
                predictions[json_obj['id']] = {'pred_text': f"{SEP_TOKEN}".join([q['question'] for q in json_obj['question_decomposition_preds']])}
    
    # generate decomposed questions
    else:
        qg_tokenized_val = val_dataset.map(qg_preprocess_function, batched=True)
        logger.info(f"generating subquestions...")
        predictions = generate_predictions(qg_tokenized_val, model, skip_special_tokens=False, num_beams=4, mode='qg')

    # save predictions to file
    out_obj = {}
    with open(pred_out_path, 'r') as f:
        for line in f:
            json_obj = json.loads(line)
            out_obj[json_obj['id']] = json_obj

    prohibitedWords = [EOS_TOKEN, PAD_TOKEN]
    big_regex = re.compile('|'.join(map(re.escape, prohibitedWords)))
    for i, (sample_id, sample) in enumerate(tqdm(predictions.items())):
        # predict supporting paragraphs for subquestions
        questions = sample["pred_text"].split(SEP_TOKEN)
        questions = [big_regex.sub("", q).strip() for q in questions]

        # store decomposed questions
        out_obj[sample_id]["question_decomposition_preds"] = [{'question': q} for q in questions]

    with open(pred_out_path, 'w') as f:
        out_obj = [v for _,v in out_obj.items()]
        f.write('\n'.join(map(json.dumps, out_obj)))
    
    logger.info(f"saved decomposed questions to {pred_out_path}")

def generate_sh_answers(pred_out_path: str, num_hops: int):
    global tokenizer

    if qa_model_checkpoint is not None:
        logger.info(f"loading model from {qa_model_checkpoint}")
        if "ft" in qa_model_checkpoint:
            model = AutoModelForSeq2SeqLM.from_pretrained(qa_model_checkpoint)
            model = model.to(device)
            tokenizer = AutoTokenizer.from_pretrained("google/long-t5-tglobal-base")
        else:
            config = PeftConfig.from_pretrained(qa_model_checkpoint)
            model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path)
            model = PeftModel.from_pretrained(model, qa_model_checkpoint)
            model = model.to(device)
            tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)

    logger.info(f"generating answers...")
    for step_idx in range(num_hops):      # assume max 4 reasoning steps
        logger.info(f"{'-'*15} Reasoning Step {step_idx}")

        # build HF dataset
        eval_dataset = build_hf_dataset(file_path=pred_out_path, step_idx=step_idx)

        if use_gold_sha:
            predictions = {sample['id']: {"pred_text": sample['gold_answer']} for sample in eval_dataset}
        else:
            # tokenize data
            tokenized_val = eval_dataset.map(qa_preprocess_function, batched=True)
            logger.info(f"Sample Input:\n{tokenizer.decode(tokenized_val['input_ids'][0])}")            
            # generate predictions
            predictions = generate_predictions(tokenized_val, model)

        # save predictions to file
        out_obj = {}
        with open(pred_out_path, 'r') as f:
            for line in f:
                json_obj = json.loads(line)
                out_obj[json_obj['id']] = json_obj

        context_ids = {sample['id']: {'context_ids': sample['context_ids'], 'context': sample['context'], 'question':sample['question']} 
                       for sample in eval_dataset}
        for sample_id, sample in predictions.items():
            out_obj[sample_id]['question_decomposition_preds'][step_idx]["predicted_answer"] = sample["pred_text"]
            out_obj[sample_id]['question_decomposition_preds'][step_idx]["paragraph_support_idx"] = context_ids[sample_id]['context_ids']
            out_obj[sample_id]['question_decomposition_preds'][step_idx]["context"] = context_ids[sample_id]['context']
            out_obj[sample_id]['question_decomposition_preds'][step_idx]["question"] = context_ids[sample_id]['question']

        with open(pred_out_path, 'w') as f:
            out_obj = [v for _,v in out_obj.items()]
            f.write('\n'.join(map(json.dumps, out_obj)))

        logger.info(f"saved predicted answers saved to: {pred_out_path}")

def get_hop_length(examples):
    num_hops = [int(id.split('_')[0][0]) for id in examples['id']]
    return {'num_hops': num_hops}

def compute_metrics(preds, labels):
    rouge_metric = eval_load('rouge')
    exact_match_metric = eval_load("exact_match")

    labels = [[l] for l in labels]

    # compute metrics
    metrics = rouge_metric.compute(predictions=preds, references=labels)
    
    em_labels = [label[0] for label in labels]
    metrics['exact_match'] = exact_match_metric.compute(predictions=preds, references=em_labels)['exact_match']

    answer_metric = AnswerMetric()
    for pred, label in list(zip(preds, labels)):
        answer_metric(pred, label)
    metrics["answer_f1"] = round(answer_metric.get_metric()[1], 3)

    return metrics

def inference(dataset, 
              pred_filename,
              num_hops=4):
    
    pred_out_path = os.path.join(logging_dir, f"{pred_filename}.jsonl")

    # duplicate original file so we can update with predicted answers
    dataset.to_json(pred_out_path)

    # decompose questions
    if args.do_predict:
        # Note the predicted decomposed questions are save in 'question_decomposition_preds' which is on the same level as the gold decompositions 'question_decomposition'
        generate_qg_decompositions(val_dataset=dataset, pred_out_path=pred_out_path)
        
        # generate sub answers
        generate_sh_answers(pred_out_path=pred_out_path, num_hops=num_hops)
    else:
        assert args.pred_path is not None, "predictions file required if 'do_predict' argument is not set to True"
        pred_out_path = args.pred_path

    # get final answer from prediction file
    logger.info(f"{'-'*15} Post-Processing")

    predictions = list()
    with open(pred_out_path, 'r') as f:
        for line in f:
            json_obj = json.loads(line)

            # pred_ans = [sample["predicted_answer"] for sample in json_obj["question_decomposition_preds"]
            #                                         if "predicted_answer" in sample.keys()][-1]
            # if json_obj['answer'].lower() not in pred_ans.lower():
            #     print(f"\n\nGT={json_obj['answer']}")
            #     print(f"Pred={pred_ans}")
                
            #     pred_cot = ". ".join([f"{hop['question']} {hop['predicted_answer']}" for hop in json_obj["question_decomposition_preds"] if "predicted_answer" in hop.keys()])
            #     print(f"Pred={pred_cot}")
                
            #     pred_gt_cot = ". ".join([f"{hop['question']} {hop['answer']}" for hop in json_obj["question_decomposition"]])
            #     print(f"GT Pred={pred_gt_cot}")

            predicted_support_idxs = [sample['paragraph_support_idx'] for sample in json_obj["question_decomposition_preds"] if 'paragraph_support_idx' in sample.keys()]
            predicted_support_idxs = list(set([idx for sample in predicted_support_idxs for idx in sample]))
            predictions.append({"id": json_obj["id"],
                                "predicted_answer": [sample["predicted_answer"] for sample in json_obj["question_decomposition_preds"]
                                                                                if "predicted_answer" in sample.keys()][-1],
                                "predicted_support_idxs": predicted_support_idxs,
                                "predicted_answerable": True
                                })

    clean_pred_out_path = os.path.join(logging_dir, f"{pred_filename}-clean.jsonl")

    with open(clean_pred_out_path, 'w+') as f:
        f.write('\n'.join(map(json.dumps, predictions)))

    logger.info(f"clean prediction file saved to: {clean_pred_out_path}")
    metrics = musique_eval_script(clean_pred_out_path, pred_out_path)
    metrics['support_recall'] = compute_retrieval_metric(clean_pred_out_path, pred_out_path)

    print(f"{metrics=}")
    logger.info(metrics)

# def main():
if __name__ == '__main__':
    global args, logging_dir, qg_model_checkpoint, qa_model_checkpoint, sentence_model, top_k_context, use_gold_shq, use_gold_context, use_gold_sha
    
    parser = ArgumentParser()
    parser.add_argument("--qa_model_checkpoint", type=str, default=None)
    parser.add_argument("--qg_model_checkpoint", type=str, default=None)
    parser.add_argument("--ir_model_checkpoint", type=str, default=None)
    parser.add_argument("--top_k_context", type=int, default=3)
    parser.add_argument("--use_gold_context", action="store_true")
    parser.add_argument("--use_gold_shq", action="store_true")
    parser.add_argument("--use_gold_sha", action="store_true")
    parser.add_argument("--eval_all", action="store_true")
    parser.add_argument("--eval_nhop", action="store_true")
    parser.add_argument("--do_predict", action="store_true")
    parser.add_argument("--pred_path", type=str, default=None)
    parser.add_argument("--qg_pred_path", type=str, default=None)
    parser.add_argument("--experiment_name", type=str, default=None)
    parser.add_argument("--base_path", type=str, default="/home/")
    args = parser.parse_args()

    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    base_path = args.base_path
    use_gold_shq = args.use_gold_shq
    use_gold_sha = args.use_gold_sha
    use_gold_context = args.use_gold_context
    top_k_context = args.top_k_context
    qg_model_checkpoint = args.qg_model_checkpoint
    qa_model_checkpoint = args.qa_model_checkpoint
    ir_model_checkpoint = args.ir_model_checkpoint
    if args.do_predict:
        if args.experiment_name is None:
            experiment_name = f"MHQAviaQG_QA=gold" if use_gold_sha else f"MHQAviaQG_QA={qa_model_checkpoint.split('/')[-1]}"
            experiment_name += f"_QG=gold" if use_gold_shq else f"_QG={qg_model_checkpoint.split('/')[-2]}"
            experiment_name += f"_IR=gold" if use_gold_context else f"_IR={ir_model_checkpoint.split('/')[-2]}"
            experiment_name += f"_p={top_k_context}"
        else:
            experiment_name = args.experiment_name
    else:
        experiment_name = args.pred_path.split("/")[-3]

    dttm = datetime.now(timezone.utc).strftime("%b%d_%H-%M-%S")
    logging_dir = os.path.join(base_path, "runs", experiment_name, f"{dttm}_abr-lab-abi")
    os.makedirs(logging_dir, exist_ok = True)
    init_logger(os.path.join(logging_dir, f"inference-logs.log"))

    logger.info(f"{logging_dir=}")
    logger.info(f"Using the following values:\n{args=}")

    # model to select context
    if ir_model_checkpoint is not None:
        if 'instructor' in ir_model_checkpoint:
            sentence_model = INSTRUCTOR(ir_model_checkpoint)
        elif 'sentence-transformers' in ir_model_checkpoint:
            sentence_model = SentenceTransformer(ir_model_checkpoint)
        elif 'cross-encoder' in ir_model_checkpoint or 'nboost' in ir_model_checkpoint:
            sentence_model = CrossEncoder(ir_model_checkpoint)

    # load dataset
    val_path = os.path.join(base_path, 'data', "musique_ans_v1.0_dev_paraphrased.jsonl")
    val_dataset = load_dataset("json", data_files=val_path, split="train")
    val_dataset = val_dataset.map(get_hop_length, batched=True)

    # entire dataset
    if args.eval_all:
        logger.info(f"QA Evaluation on all questions")
        pred_filename = f"{experiment_name}-all"
        inference(val_dataset, pred_filename)

    # per hop
    if args.eval_nhop:
        for num_hops in range(2,3):
            logger.info(f"QA Evaluation on {num_hops}-hop questions")
            nhop_dataset = val_dataset.filter(lambda example: example["num_hops"]==num_hops, load_from_cache_file=False)

            pred_filename = f"{experiment_name}-{num_hops}hop"
            inference(nhop_dataset, pred_filename, num_hops=num_hops)

# if __name__ == '__main__':
    # main()